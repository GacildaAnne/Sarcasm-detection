{"cells":[{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11286,"status":"ok","timestamp":1671273876808,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"zqjSgttHRyVl","outputId":"d0df5da4-7d03-4099-d3f1-9f8c6f2d0a23"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in c:\\users\\gacil\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.25.1)\n","Requirement already satisfied: filelock in c:\\users\\gacil\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (3.8.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\gacil\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.11.1)\n","Requirement already satisfied: numpy>=1.17 in c:\\users\\gacil\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (1.23.3)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\gacil\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gacil\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gacil\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in c:\\users\\gacil\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.28.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\gacil\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.13.2)\n","Requirement already satisfied: tqdm>=4.27 in c:\\users\\gacil\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (4.64.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gacil\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\gacil\\appdata\\roaming\\python\\python39\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: colorama in c:\\users\\gacil\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n","Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\gacil\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gacil\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\gacil\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (1.26.13)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gacil\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2022.9.24)\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 24.0 -> 24.2\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":10559,"status":"ok","timestamp":1671273890078,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"LCKXqXdnOnPL"},"outputs":[],"source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","\n","from tabulate import tabulate\n","from tqdm import trange\n","import random"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":2688,"status":"ok","timestamp":1671273896185,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"EoyOuL2NOnPL"},"outputs":[],"source":["GEN_DF = pd.read_csv(r\"/Users/Gacil/OneDrive/Dokumenter/CogSciKA1/NLP/Exam/SDcode/DATAsarcasm_v2/sarcasm_v2/GEN-sarc-notsarc.csv\")\n","GEN_DF.head()\n","HYP_DF = pd.read_csv(r\"/Users/Gacil/OneDrive/Dokumenter/CogSciKA1/NLP/Exam/SDcode/DATAsarcasm_v2/sarcasm_v2/HYP-sarc-notsarc.csv\")\n","\n","RQ_DF = pd.read_csv(r\"/Users/Gacil/OneDrive/Dokumenter/CogSciKA1/NLP/Exam/SDcode/DATAsarcasm_v2/sarcasm_v2/RQ-sarc-notsarc.csv\")\n"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":279,"status":"ok","timestamp":1671273898869,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"tchMEwgoOnPM","outputId":"cf17bd88-f0d0-4018-f532-f4d6984f3b73"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>class</th>\n","      <th>id</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>Archie, the ONLY issue that gays don't have a ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>No, not really. All that is different is the n...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>It's ashame that everyone keeps looking for th...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>Almost? Usually, that is true, and it involves...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>And so have animals. Plants have been wiped ou...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   class  id                                               text\n","0      0   1  Archie, the ONLY issue that gays don't have a ...\n","1      0   2  No, not really. All that is different is the n...\n","2      0   3  It's ashame that everyone keeps looking for th...\n","3      0   4  Almost? Usually, that is true, and it involves...\n","4      0   5  And so have animals. Plants have been wiped ou..."]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["# Turn 'sarc' = 1 and 'notsarc' = 0\n","GEN_DF['class'] = GEN_DF['class'].map({'sarc': 1, 'notsarc': 0})\n","GEN_DF.head()\n","\n","HYP_DF['class'] = HYP_DF['class'].map({'sarc': 1, 'notsarc': 0})\n","HYP_DF.head()\n","\n","RQ_DF['class'] = RQ_DF['class'].map({'sarc': 1 , 'notsarc': 0})\n","RQ_DF.head()"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":310,"status":"ok","timestamp":1671273903077,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"miR0XbrfOnPN"},"outputs":[],"source":["# Change column name from class to label\n","GEN_DF.columns = ['label', 'id', 'text']\n","HYP_DF.columns = ['label', 'id', 'text']\n","RQ_DF.columns = ['label', 'id', 'text']\n"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1671273905316,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"g0jRn_NBXWdg","outputId":"7ed5dca8-885f-426f-b812-81bed0fbd149"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>id</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>If that's true, then Freedom of Speech is doom...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>Neener neener - is it time to go in from the p...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Just like the plastic gun fear, the armour pie...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>So geology is a religion because we weren't he...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>Well done Monty. Mark that up as your first ev...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   label  id                                               text\n","0      0   1  If that's true, then Freedom of Speech is doom...\n","1      0   2  Neener neener - is it time to go in from the p...\n","2      0   3  Just like the plastic gun fear, the armour pie...\n","3      0   4  So geology is a religion because we weren't he...\n","4      0   5  Well done Monty. Mark that up as your first ev..."]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["# Make a separate dataframe with all the datatypes combined\n","\n","ALL_DF = pd.concat([GEN_DF, HYP_DF, RQ_DF])\n","\n","ALL_DF.head()"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":306,"status":"ok","timestamp":1671273909221,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"uVcBpR3tOnPN"},"outputs":[],"source":["# Shuffle\n","from sklearn.utils import shuffle\n","\n","GEN_DF = shuffle(GEN_DF, random_state = 1)\n","HYP_DF = shuffle(HYP_DF, random_state = 1)\n","RQ_DF = shuffle(RQ_DF, random_state = 1)\n","ALL_DF = shuffle(ALL_DF, random_state = 1)\n"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["2a5d6ae85cb845b3b9933c35b1a5501a","ac7c3514813f404a9f1fc87a04e88c57","78dd9cff75b54f2a8967a61fd657e840","91cd6260e1c146eaab28a1fe90a0250b","e98c9f6fb08b43c3b75bb28b7ed89683","a52d83281e604b748953668b4b66a9f9","56d5c9265bbf487088e7e9f15396b05d","a8f0702a5fc143959f2fdfb4ce0a4130","3c9f4a7bae40438e8a08b3df301baf06","0728c01761374afe963e86eba8fab3ae","9547a27d702148ddbde6ec4ed8dbcfea","83bf836aa925473d831aee36f5031ef2","562f8eca86da44e282a55d8442afac92","f19724243b7b46f6bf07aac17e57f5a4","09e6b8cd29b34897a898973508febef1","8b4fcd200a7b4e3bb9e11938c3b0cdcb","2a58fac03b184dc3ae88c4e22ecc50f8","973776c0b5ec45d0a2ae180534611df1","8a5bb0a0ea3e40f196f10e6f72136cf5","fdf41ea32c50487a90baacf28cd75ca6","11623e2c1c8d4efd8534f1c224c75a30","ee053d61bdda439f84f37333fd3a2306","a7c966949a084c75a02b3113671949e7","9f54c3c2bea441f289ea1c5f5eb3ab2a","99a0d8bade97483b8df7481da1604ef4","c1f8c82574e54d1cba1de934843d9a48","181160ca9c7b4e27ab8539fcec68d7fb","cad6ef30082040669140f737018db588","a7721104836a436da78c222207d54b06","3fa92f9f57d741f08fc6054aba22b7f9","21274bd2d6684e5aafd785871599f171","f2347bab25ea4af8ac364ca596d893b9","a7924494dd4a469096b1e6316746843d"]},"executionInfo":{"elapsed":2702,"status":"ok","timestamp":1671273915869,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"YBPOTX8YOnPO","outputId":"f1d27fd2-f713-4eeb-8a30-e96d4cd4fafb"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"15d2f431c6a94a509c542d1fd74f0b95","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Gacil\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Gacil\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n","To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n","  warnings.warn(message)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2f44ca63aad64f179a28786aa3c14195","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4f55db0bc85f43c3b88970cf2d73e75e","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Download BERT tokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained(\n","    'bert-base-uncased',\n","    do_lower_case = True\n","    )"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1671204443553,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"hasvU47bOnPO"},"outputs":[],"source":["### This code prints a random sentence from the dataset in a table with\n","### the tokens and token IDs, so you can see how it looks.\n","\n","# def print_rand_sentence():\n","#   '''Displays the tokens and respective IDs of a random text sample'''\n","#   index = random.randint(0, len(GENtext)-1)\n","#   table = np.array([tokenizer.tokenize(GENtext[index]), \n","#                     tokenizer.convert_tokens_to_ids(tokenizer.tokenize(GENtext[index]))]).T\n","#   print(tabulate(table,\n","#                  headers = ['Tokens', 'Token IDs'],\n","#                  tablefmt = 'fancy_grid'))\n","\n","# print_rand_sentence()"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1671204443555,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"dVHyNDGKOnPP"},"outputs":[],"source":["# # Count maximum number of words in text column in each dataframe \n","# ## answer = 150\n","\n","# max(GEN_DF[\"text\"].apply(lambda n: len(n.split())))\n","# max(HYP_DF[\"text\"].apply(lambda n: len(n.split())))\n","# max(RQ_DF[\"text\"].apply(lambda n: len(n.split())))"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":262,"status":"ok","timestamp":1671273924083,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"OXg-1iodFOm2"},"outputs":[],"source":["# Specify ratios for training, validation and testin\n","TV_ratio = 0.2\n","val_ratio = 0.5 # Thus the test ratio is 0.5\n"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":266,"status":"ok","timestamp":1671273926666,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"wETu2BiSmztX"},"outputs":[],"source":["# Split GEN\n","train_G, TV_G = train_test_split(\n","    GEN_DF,\n","    test_size = TV_ratio,\n","    shuffle = False)\n","\n","test_G, val_G = train_test_split(\n","    TV_G,\n","    test_size = val_ratio,\n","    shuffle = False)\n","\n","# Split HYP\n","train_H, TV_H = train_test_split(\n","    HYP_DF,\n","    test_size = TV_ratio,\n","    shuffle = False)\n","\n","test_H, val_H = train_test_split(\n","    TV_H,\n","    test_size = val_ratio,\n","    shuffle = False)\n","\n","# Split RQ\n","train_R, TV_R = train_test_split(\n","    RQ_DF,\n","    test_size = TV_ratio,\n","    shuffle = False)\n","\n","test_R, val_R = train_test_split(\n","    TV_R,\n","    test_size = val_ratio,\n","    shuffle = False)\n","\n","\n","# Split ALL\n","train_ALL, TV_ALL = train_test_split(\n","    ALL_DF,\n","    test_size = TV_ratio,\n","    shuffle = False)\n","\n","test_ALL, val_ALL = train_test_split(\n","    TV_ALL,\n","    test_size = val_ratio,\n","    shuffle = False)\n"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":238,"status":"ok","timestamp":1671273930962,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"FGEmjoU1l3GQ"},"outputs":[],"source":["# Get text and label values of TRAIN data\n","\n","GENtext_tr = train_G.text.values\n","GENlabel_tr = train_G.label.values\n","\n","HYPtext_tr = train_H.text.values\n","HYPlabel_tr = train_H.label.values\n","\n","RQtext_tr = train_R.text.values\n","RQlabel_tr = train_R.label.values\n","\n","ALLtext_tr = train_ALL.text.values\n","ALLlabel_tr = train_ALL.label.values"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":271,"status":"ok","timestamp":1671273933975,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"Iuc7ApWUnBTU"},"outputs":[],"source":["# Get text and label values of VAL data\n","\n","GENtext_val = val_G.text.values\n","GENlabel_val = val_G.label.values\n","\n","HYPtext_val = val_H.text.values\n","HYPlabel_val = val_H.label.values\n","\n","RQtext_val = val_R.text.values\n","RQlabel_val = val_R.label.values\n","\n","ALLtext_val = val_ALL.text.values\n","ALLlabel_val = val_ALL.label.values"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":244,"status":"ok","timestamp":1671273937033,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"ny70VHxnrUYC"},"outputs":[],"source":["# Get text and label values of TEST data\n","\n","GENtext_test = test_G.text.values\n","GENlabel_test = test_G.label.values\n","\n","HYPtext_test = test_H.text.values\n","HYPlabel_test = test_H.label.values\n","\n","RQtext_test = test_R.text.values\n","RQlabel_test = test_R.label.values\n","\n","ALLtext_test = test_ALL.text.values\n","ALLlabel_test = test_ALL.label.values"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29267,"status":"ok","timestamp":1671273974533,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"sZf6EK3fOnPQ","outputId":"24a535e4-af6d-48e5-f266-bc8ed60c0452"},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","c:\\Users\\Gacil\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}],"source":["### Pre-processing for BERT\n","\n","train_id_G = []\n","train_masks_G = []\n","val_id_G = []\n","val_masks_G = []\n","test_id_G = []\n","test_masks_G = []\n","\n","train_id_H = []\n","train_masks_H = []\n","val_id_H = []\n","val_masks_H = []\n","test_id_H = []\n","test_masks_H = []\n","\n","train_id_R = []\n","train_masks_R = []\n","val_id_R = []\n","val_masks_R = []\n","test_id_R = []\n","test_masks_R = []\n","\n","train_id_ALL = []\n","train_masks_ALL = []\n","val_id_ALL = []\n","val_masks_ALL = []\n","test_id_ALL = []\n","test_masks_ALL = []\n","\n","\n","# Define preprocessing code\n","def preprocessing(input_text, tokenizer):\n","  '''\n","  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n","    - input_ids: list of token ids\n","    - token_type_ids: list of token type ids\n","    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n","  '''\n","  return tokenizer.encode_plus(\n","                        input_text,\n","                        add_special_tokens = True,\n","                        max_length = 150,\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,\n","                        return_tensors = 'pt'\n","                   )\n","\n","\n","\n","\n","# Loop preprocessing function over GEN train text data\n","for sample in GENtext_tr:\n","  encoding_dict = preprocessing(sample, tokenizer)\n","  train_id_G.append(encoding_dict['input_ids']) \n","  train_masks_G.append(encoding_dict['attention_mask'])\n","\n","train_id_G = torch.cat(train_id_G, dim = 0)\n","train_masks_G = torch.cat(train_masks_G, dim = 0)\n","# GEN train label data\n","labelsGEN_tr = torch.tensor(GENlabel_tr)\n","\n","\n","\n","# Loop preprocessing function over GEN val text data\n","for sample in GENtext_val:\n","  encoding_dict = preprocessing(sample, tokenizer)\n","  val_id_G.append(encoding_dict['input_ids']) \n","  val_masks_G.append(encoding_dict['attention_mask'])\n","\n","val_id_G = torch.cat(val_id_G, dim = 0)\n","val_masks_G = torch.cat(val_masks_G, dim = 0)\n","# GEN val label data\n","labelsGEN_val = torch.tensor(GENlabel_val)\n","\n","\n","\n","# Loop preprocessing function over GEN test text data\n","for sample in GENtext_test:\n","  encoding_dict = preprocessing(sample, tokenizer)\n","  test_id_G.append(encoding_dict['input_ids']) \n","  test_masks_G.append(encoding_dict['attention_mask'])\n","\n","test_id_G = torch.cat(test_id_G, dim = 0)\n","test_masks_G = torch.cat(test_masks_G, dim = 0)\n","# GEN test label data\n","labelsGEN_test = torch.tensor(GENlabel_test)\n","\n","\n","\n","\n","\n","\n","\n","\n","# Loop preprocessing function over HYP train text data\n","for sample in HYPtext_tr:\n","  encoding_dict = preprocessing(sample, tokenizer)\n","  train_id_H.append(encoding_dict['input_ids']) \n","  train_masks_H.append(encoding_dict['attention_mask'])\n","\n","train_id_H = torch.cat(train_id_H, dim = 0)\n","train_masks_H = torch.cat(train_masks_H, dim = 0)\n","# HYP train label data\n","labelsHYP_tr = torch.tensor(HYPlabel_tr)\n","\n","\n","# Loop preprocessing function over HYP val text data\n","for sample in HYPtext_val:\n","  encoding_dict = preprocessing(sample, tokenizer)\n","  val_id_H.append(encoding_dict['input_ids']) \n","  val_masks_H.append(encoding_dict['attention_mask'])\n","\n","val_id_H = torch.cat(val_id_H, dim = 0)\n","val_masks_H = torch.cat(val_masks_H, dim = 0)\n","# HYP val label data\n","labelsHYP_val = torch.tensor(HYPlabel_val)\n","\n","\n","# Loop preprocessing function over HYP test text data\n","for sample in HYPtext_test:\n","  encoding_dict = preprocessing(sample, tokenizer)\n","  test_id_H.append(encoding_dict['input_ids']) \n","  test_masks_H.append(encoding_dict['attention_mask'])\n","\n","test_id_H = torch.cat(test_id_H, dim = 0)\n","test_masks_H = torch.cat(test_masks_H, dim = 0)\n","# HYP test label data\n","labelsHYP_test = torch.tensor(HYPlabel_test)\n","\n","\n","\n","\n","\n","\n","\n","\n","# Loop preprocessing function over RQ train text data\n","for sample in RQtext_tr:\n","  encoding_dict = preprocessing(sample, tokenizer)\n","  train_id_R.append(encoding_dict['input_ids']) \n","  train_masks_R.append(encoding_dict['attention_mask'])\n","\n","train_id_R = torch.cat(train_id_R, dim = 0)\n","train_masks_R = torch.cat(train_masks_R, dim = 0)\n","# RQ train label data\n","labelsRQ_tr = torch.tensor(RQlabel_tr)\n","\n","\n","\n","# Loop preprocessing function over RQ val text data\n","for sample in RQtext_val:\n","  encoding_dict = preprocessing(sample, tokenizer)\n","  val_id_R.append(encoding_dict['input_ids']) \n","  val_masks_R.append(encoding_dict['attention_mask'])\n","\n","val_id_R = torch.cat(val_id_R, dim = 0)\n","val_masks_R = torch.cat(val_masks_R, dim = 0)\n","# RQ val label data\n","labelsRQ_val = torch.tensor(RQlabel_val)\n","\n","\n","\n","# Loop preprocessing function over RQ test text data\n","for sample in RQtext_test:\n","  encoding_dict = preprocessing(sample, tokenizer)\n","  test_id_R.append(encoding_dict['input_ids']) \n","  test_masks_R.append(encoding_dict['attention_mask'])\n","\n","test_id_R = torch.cat(test_id_R, dim = 0)\n","test_masks_R = torch.cat(test_masks_R, dim = 0)\n","# RQ test label data\n","labelsRQ_test = torch.tensor(RQlabel_test)\n","\n","\n","\n","\n","\n","\n","\n","\n","# Loop preprocessing function over ALL train text data\n","for sample in ALLtext_tr:\n","  encoding_dict = preprocessing(sample, tokenizer)\n","  train_id_ALL.append(encoding_dict['input_ids']) \n","  train_masks_ALL.append(encoding_dict['attention_mask'])\n","\n","train_id_ALL = torch.cat(train_id_ALL, dim = 0)\n","train_masks_ALL = torch.cat(train_masks_ALL, dim = 0)\n","# RQ train label data\n","labelsALL_tr = torch.tensor(ALLlabel_tr)\n","\n","\n","\n","# Loop preprocessing function over RQ val text data\n","for sample in ALLtext_val:\n","  encoding_dict = preprocessing(sample, tokenizer)\n","  val_id_ALL.append(encoding_dict['input_ids']) \n","  val_masks_ALL.append(encoding_dict['attention_mask'])\n","\n","val_id_ALL = torch.cat(val_id_ALL, dim = 0)\n","val_masks_ALL = torch.cat(val_masks_ALL, dim = 0)\n","# RQ val label data\n","labelsALL_val = torch.tensor(ALLlabel_val)\n","\n","\n","\n","# Loop preprocessing function over RQ test text data\n","for sample in ALLtext_test:\n","  encoding_dict = preprocessing(sample, tokenizer)\n","  test_id_ALL.append(encoding_dict['input_ids']) \n","  test_masks_ALL.append(encoding_dict['attention_mask'])\n","\n","test_id_ALL = torch.cat(test_id_ALL, dim = 0)\n","test_masks_ALL = torch.cat(test_masks_ALL, dim = 0)\n","# RQ test label data\n","labelsALL_test = torch.tensor(ALLlabel_test)\n"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":266,"status":"ok","timestamp":1671274065798,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"__PvWxB_OnPQ","outputId":"8e501eb2-d051-4a51-86cb-a49071dd7b5e"},"outputs":[{"data":{"text/plain":["tensor(1)"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["# Checking\n","\n","labelsGEN_tr[32]"]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1671204479763,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"cwt_g_SSpTeU"},"outputs":[],"source":["# Turn train_idx and val_idx into numpy arrays\n","\n","\n"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":295,"status":"ok","timestamp":1671274075586,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"V1fzD2vWOnPV"},"outputs":[],"source":["\n","batch_size = 20\n","\n","# GEN\n","\n","# Train and validation sets\n","train_setG = TensorDataset(train_id_G, \n","                          train_masks_G, \n","                          labelsGEN_tr)\n","\n","val_setG = TensorDataset(val_id_G, \n","                        val_masks_G, \n","                        labelsGEN_val)\n","\n","\n","\n","# Prepare DataLoader\n","train_dataloaderG = DataLoader(\n","            train_setG,\n","            sampler = RandomSampler(train_setG),\n","            batch_size = batch_size\n","        )\n","\n","validation_dataloaderG = DataLoader(\n","            val_setG,\n","            sampler = SequentialSampler(val_setG),\n","            batch_size = batch_size\n","        )"]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1671274099581,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"dci9aqBRsx7D"},"outputs":[],"source":["# HYP\n","\n","# Train and validation sets\n","train_setH = TensorDataset(train_id_H, \n","                          train_masks_H, \n","                          labelsHYP_tr)\n","\n","val_setH = TensorDataset(val_id_H, \n","                        val_masks_H, \n","                        labelsHYP_val)\n","\n","\n","\n","# Prepare DataLoader\n","train_dataloaderH = DataLoader(\n","            train_setH,\n","            sampler = RandomSampler(train_setH),\n","            batch_size = batch_size\n","        )\n","\n","validation_dataloaderH = DataLoader(\n","            val_setH,\n","            sampler = SequentialSampler(val_setH),\n","            batch_size = batch_size\n","        )"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":265,"status":"ok","timestamp":1671274109380,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"SxuGomNQfBzY"},"outputs":[],"source":["# RQ\n","\n","# Train and validation sets\n","train_setR = TensorDataset(train_id_R, \n","                          train_masks_R, \n","                          labelsRQ_tr)\n","\n","val_setR = TensorDataset(val_id_R, \n","                        val_masks_R, \n","                        labelsRQ_val)\n","\n","\n","\n","# Prepare DataLoader\n","train_dataloaderR = DataLoader(\n","            train_setR,\n","            sampler = RandomSampler(train_setR),\n","            batch_size = batch_size\n","        )\n","\n","validation_dataloaderR = DataLoader(\n","            val_setR,\n","            sampler = SequentialSampler(val_setR),\n","            batch_size = batch_size\n","        )"]},{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":298,"status":"ok","timestamp":1671274113406,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"BtlA5JEBiKwS"},"outputs":[],"source":["# ALL\n","\n","# Train and validation sets\n","train_setALL = TensorDataset(train_id_ALL, \n","                          train_masks_ALL, \n","                          labelsALL_tr)\n","\n","val_setALL = TensorDataset(val_id_ALL, \n","                        val_masks_ALL, \n","                        labelsALL_val)\n","\n","\n","\n","# Prepare DataLoader\n","train_dataloaderALL = DataLoader(\n","            train_setALL,\n","            sampler = RandomSampler(train_setALL),\n","            batch_size = batch_size\n","        )\n","\n","validation_dataloaderALL = DataLoader(\n","            val_setALL,\n","            sampler = SequentialSampler(val_setALL),\n","            batch_size = batch_size\n","        )"]},{"cell_type":"markdown","metadata":{"id":"MLCwbOduOnPV"},"source":["Create functions for assessing validation metrics (accuracy, precision, recall, specificity)\n"]},{"cell_type":"code","execution_count":45,"metadata":{"executionInfo":{"elapsed":373,"status":"ok","timestamp":1671274119417,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"qnETXyX3OnPV"},"outputs":[],"source":["\n","def b_tp(preds, labels):\n","  '''Returns True Positives (TP): count of correct predictions of actual class 1'''\n","  return sum([preds == labels and preds == 1 for preds, labels in zip(preds, labels)])\n","\n","def b_fp(preds, labels):\n","  '''Returns False Positives (FP): count of wrong predictions of actual class 1'''\n","  return sum([preds != labels and preds == 1 for preds, labels in zip(preds, labels)])\n","\n","def b_tn(preds, labels):\n","  '''Returns True Negatives (TN): count of correct predictions of actual class 0'''\n","  return sum([preds == labels and preds == 0 for preds, labels in zip(preds, labels)])\n","\n","def b_fn(preds, labels):\n","  '''Returns False Negatives (FN): count of wrong predictions of actual class 0'''\n","  return sum([preds != labels and preds == 0 for preds, labels in zip(preds, labels)])\n","\n","def b_metrics(preds, labels):\n","  '''\n","  Returns the following metrics:\n","    - accuracy    = (TP + TN) / N\n","    - precision   = TP / (TP + FP)\n","    - recall      = TP / (TP + FN)\n","    - specificity = TN / (TN + FP)\n","  '''\n","  preds = np.argmax(preds, axis = 1).flatten()\n","  labels = labels.flatten()\n","  tp = b_tp(preds, labels)\n","  tn = b_tn(preds, labels)\n","  fp = b_fp(preds, labels)\n","  fn = b_fn(preds, labels)\n","  b_accuracy = (tp + tn) / len(labels)\n","  b_precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n","  b_recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n","  b_specificity = tn / (tn + fp) if (tn + fp) > 0 else 'nan'\n","  return b_accuracy, b_precision, b_recall, b_specificity"]},{"cell_type":"markdown","metadata":{"id":"S4eq1RVYOnPY"},"source":["Load BERT for sequence classification model"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6825,"status":"ok","timestamp":1671204486570,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"PbVJCoc1OnPY","outputId":"d7b3651e-c970-4129-f02b-bcd61b1823dd"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a021e27a92bb4bcab1d619a0cb305199","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Load the BertForSequenceClassification model1 for GEN \n","model1 = BertForSequenceClassification.from_pretrained(\n","    'bert-base-uncased',\n","    num_labels = 2,\n","    output_attentions = False,\n","    output_hidden_states = False,\n",")\n","\n","# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5. See: https://arxiv.org/pdf/1810.04805.pdf\n","optimizer1 = torch.optim.AdamW(model1.parameters(), \n","                              lr = 1e-5,\n","                              eps = 1e-08\n","                              )\n","\n","\n","\n","\n","# Load the BertForSequenceClassification model2 for HYP\n","model2 = BertForSequenceClassification.from_pretrained(\n","    'bert-base-uncased',\n","    num_labels = 2,\n","    output_attentions = False,\n","    output_hidden_states = False,\n",")\n","\n","# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5. See: https://arxiv.org/pdf/1810.04805.pdf\n","optimizer2 = torch.optim.AdamW(model2.parameters(), \n","                              lr = 1e-5,\n","                              eps = 1e-08\n","                              )\n","\n","\n","\n","# Load the BertForSequenceClassification model3 for RQ\n","model3 = BertForSequenceClassification.from_pretrained(\n","    'bert-base-uncased',\n","    num_labels = 2,\n","    output_attentions = False,\n","    output_hidden_states = False,\n",")\n","\n","# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5. See: https://arxiv.org/pdf/1810.04805.pdf\n","optimizer3 = torch.optim.AdamW(model3.parameters(), \n","                              lr = 1e-5,\n","                              eps = 1e-08\n","                              )\n","\n","\n","\n","# Load the BertForSequenceClassification model4 for ALL DATA\n","model4 = BertForSequenceClassification.from_pretrained(\n","    'bert-base-uncased',\n","    num_labels = 2,\n","    output_attentions = False,\n","    output_hidden_states = False,\n",")\n","\n","# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5. See: https://arxiv.org/pdf/1810.04805.pdf\n","optimizer4 = torch.optim.AdamW(model4.parameters(), \n","                              lr = 1e-5,\n","                              eps = 1e-08\n","                              )\n","\n"]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1671204486571,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"gZ_4Xs26GaPG"},"outputs":[],"source":[" torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"S68Ia1Ehghvn"},"source":["We start with training GEN"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":573456,"status":"ok","timestamp":1671205060015,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"eli9NlvjOnPZ","outputId":"23db0aba-3a3f-4b05-8ce0-378309182021"},"outputs":[{"ename":"AssertionError","evalue":"Torch not compiled with CUDA enabled","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[50], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Recommended number of epochs: 2, 3, 4. See: https://arxiv.org/pdf/1810.04805.pdf\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Gacil\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:747\u001b[0m, in \u001b[0;36mModule.cuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    732\u001b[0m \n\u001b[0;32m    733\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Gacil\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 639\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    642\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    643\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    644\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    650\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Gacil\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 639\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    642\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    643\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    644\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    650\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Gacil\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 639\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    642\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    643\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    644\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    650\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Gacil\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:662\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 662\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    663\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n","File \u001b[1;32mc:\\Users\\Gacil\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:747\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    732\u001b[0m \n\u001b[0;32m    733\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[1;32mc:\\Users\\Gacil\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\cuda\\__init__.py:221\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"]}],"source":["model1.cuda()\n","\n","device = torch.device('cuda')\n","\n","# Recommended number of epochs: 2, 3, 4. See: https://arxiv.org/pdf/1810.04805.pdf\n","epochs = 4\n","print(\"[INFO:] Training classifier on GEN data...\")\n","\n","for _ in trange(epochs, desc = 'Epoch'):\n","    \n","    # ========== Training ==========\n","    \n","    # Set model to training mode\n","    model1.train()\n","    \n","    # Tracking variables\n","    tr_loss = 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","\n","    for step, batch in enumerate(train_dataloaderG):\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        optimizer1.zero_grad()\n","        # Forward pass\n","        train_output_GEN = model1(b_input_ids, \n","                             token_type_ids = None, \n","                             attention_mask = b_input_mask, \n","                             labels = b_labels)\n","        # Backward pass\n","        train_output_GEN.loss.backward()\n","        optimizer1.step()\n","        # Update tracking variables\n","        tr_loss += train_output_GEN.loss.item()\n","        nb_tr_examples += b_input_ids.size(0)\n","        nb_tr_steps += 1\n","\n","# ========== Validation ==========\n","\n","    # Set model to evaluation mode\n","    model1.eval()\n","\n","    # Tracking variables \n","    val_accuracy = []\n","    val_precision = []\n","    val_recall = []\n","    val_specificity = []\n","\n","    for batch in validation_dataloaderG:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        with torch.no_grad():\n","          # Forward pass\n","          eval_output_GEN = model1(b_input_ids, \n","                              token_type_ids = None, \n","                              attention_mask = b_input_mask)\n","        logits_GEN = eval_output_GEN.logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        # Calculate validation metrics\n","        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits_GEN, label_ids)\n","        val_accuracy.append(b_accuracy)\n","        # Update precision only when (tp + fp) !=0; ignore nan\n","        if b_precision != 'nan': val_precision.append(b_precision)\n","        # Update recall only when (tp + fn) !=0; ignore nan\n","        if b_recall != 'nan': val_recall.append(b_recall)\n","        # Update specificity only when (tn + fp) !=0; ignore nan\n","        if b_specificity != 'nan': val_specificity.append(b_specificity)\n","\n","    print('\\n\\t - GEN Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n","    print('\\t - GEN Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n","    print('\\t - GEN Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n","    print('\\t - GEN Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n","    print('\\t - GEN Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')\n"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":4945,"status":"ok","timestamp":1671205064950,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"QYXpT7gycr7D"},"outputs":[],"source":["# Save the entire model to a file (model1)\n","torch.save({'model_state_dict': model1.state_dict(),\n","            'optimizer_state_dict': optimizer1.state_dict()}, 'model1.pth')\n"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":218},"executionInfo":{"elapsed":377,"status":"error","timestamp":1671274332214,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"YFIaW-0js650","outputId":"cc010121-6c92-4612-f7aa-22cb42ba32dd"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-ee24be29b1bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load model1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m model1 = torch.load(('model1.pth'),model1.load_state_dict(model1['model_state_dict']),\n\u001b[0m\u001b[1;32m      4\u001b[0m optimizer1.load_state_dict(model1['optimizer1_state_dict']))\n","\u001b[0;31mNameError\u001b[0m: name 'model1' is not defined"]}],"source":["# Load model1\n","\n","model1 = torch.load(('model1.pth'),model1.load_state_dict(model1['model_state_dict']),\n","optimizer1.load_state_dict(model1['optimizer1_state_dict']))"]},{"cell_type":"markdown","metadata":{"id":"ArVWUjKy9KWT"},"source":["This model can be loaded again by writing:\n","\n","\"\n","import torch\n","\n","# Load the model and optimizer's state\n","model1 = torch.load('model1.pth')\n","model.load_state_dict(model1['model_state_dict'])\n","optimizer.load_state_dict(model1['optimizer1_state_dict'])\n","\n","# Use the model and optimizer to continue training or make predictions\n","\""]},{"cell_type":"markdown","metadata":{"id":"MSDMp8-PUQE1"},"source":["Now train HYP"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":103315,"status":"ok","timestamp":1671205168257,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"bL9ExCcqUHpR","outputId":"e3c74ebb-9f0f-4f8e-8d55-0b3f9345b103"},"outputs":[{"name":"stdout","output_type":"stream","text":["[INFO:] Training classifier on HYP data...\n"]},{"name":"stderr","output_type":"stream","text":["Epoch:  25%|       | 1/4 [00:25<01:17, 25.82s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","\t - HYP Train loss: 0.6878\n","\t - HYP Validation Accuracy: 0.5951\n","\t - HYP Validation Precision: 0.6890\n","\t - HYP Validation Recall: 0.5818\n","\t - HYP Validation Specificity: 0.6095\n","\n"]},{"name":"stderr","output_type":"stream","text":["\rEpoch:  50%|     | 2/4 [00:51<00:51, 25.93s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","\t - HYP Train loss: 0.6166\n","\t - HYP Validation Accuracy: 0.7025\n","\t - HYP Validation Precision: 0.8049\n","\t - HYP Validation Recall: 0.6691\n","\t - HYP Validation Specificity: 0.7520\n","\n"]},{"name":"stderr","output_type":"stream","text":["\rEpoch:  75%|  | 3/4 [01:17<00:25, 25.75s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","\t - HYP Train loss: 0.4998\n","\t - HYP Validation Accuracy: 0.6593\n","\t - HYP Validation Precision: 0.7564\n","\t - HYP Validation Recall: 0.6272\n","\t - HYP Validation Specificity: 0.6981\n","\n"]},{"name":"stderr","output_type":"stream","text":["Epoch: 100%|| 4/4 [01:43<00:00, 25.77s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","\t - HYP Train loss: 0.3512\n","\t - HYP Validation Accuracy: 0.7554\n","\t - HYP Validation Precision: 0.8304\n","\t - HYP Validation Recall: 0.7461\n","\t - HYP Validation Specificity: 0.7705\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["model2.cuda() \n","\n","print(\"[INFO:] Training classifier on HYP data...\")\n","\n","for _ in trange(epochs, desc = 'Epoch'):\n","    \n","    # ========== Training ==========\n","    \n","    # Set model to training mode\n","    model2.train()\n","    \n","    # Tracking variables\n","    tr_loss = 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","\n","    for step, batch in enumerate(train_dataloaderH):\n","        batch = tuple(t.to(device) for t in batch)\n","        c_input_ids, c_input_mask, c_labels = batch\n","        optimizer2.zero_grad()\n","        # Forward pass\n","        train_output_HYP = model2(c_input_ids, \n","                             token_type_ids = None, \n","                             attention_mask = c_input_mask, \n","                             labels = c_labels)\n","        # Backward pass\n","        train_output_HYP.loss.backward()\n","        optimizer2.step()\n","        # Update tracking variables\n","        tr_loss += train_output_HYP.loss.item()\n","        nb_tr_examples += c_input_ids.size(0)\n","        nb_tr_steps += 1\n","\n","# ========== Validation ==========\n","\n","    # Set model to evaluation mode\n","    model2.eval()\n","\n","    # Tracking variables \n","    val_accuracy = []\n","    val_precision = []\n","    val_recall = []\n","    val_specificity = []\n","\n","    for batch in validation_dataloaderH:\n","        batch = tuple(t.to(device) for t in batch)\n","        c_input_ids, c_input_mask, c_labels = batch\n","        with torch.no_grad():\n","          # Forward pass\n","          eval_output_HYP = model2(c_input_ids, \n","                              token_type_ids = None, \n","                              attention_mask = c_input_mask)\n","        logits_HYP = eval_output_HYP.logits.detach().cpu().numpy()\n","        label_ids = c_labels.to('cpu').numpy()\n","        # Calculate validation metrics\n","        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits_HYP, label_ids)\n","        val_accuracy.append(b_accuracy)\n","        # Update precision only when (tp + fp) !=0; ignore nan\n","        if b_precision != 'nan': val_precision.append(b_precision)\n","        # Update recall only when (tp + fn) !=0; ignore nan\n","        if b_recall != 'nan': val_recall.append(b_recall)\n","        # Update specificity only when (tn + fp) !=0; ignore nan\n","        if b_specificity != 'nan': val_specificity.append(b_specificity)\n","\n","    print('\\n\\t - HYP Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n","    print('\\t - HYP Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n","    print('\\t - HYP Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n","    print('\\t - HYP Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n","    print('\\t - HYP Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')\n"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":4714,"status":"ok","timestamp":1671205172958,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"v-4ORvN2-VsD"},"outputs":[],"source":["# Save the entire model to a file (model1)\n","torch.save({'model_state_dict': model2.state_dict(),\n","            'optimizer_state_dict': optimizer2.state_dict()}, 'model2.pth')"]},{"cell_type":"markdown","metadata":{"id":"HMo6hQR7USDa"},"source":["Now train RQ"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":152853,"status":"ok","timestamp":1671205325801,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"9fvNViqMUg1V","outputId":"f1007830-9dfe-4b95-ad76-5126fedd828e"},"outputs":[{"name":"stdout","output_type":"stream","text":["[INFO:] Training classifier on RQ data...\n"]},{"name":"stderr","output_type":"stream","text":["Epoch:  25%|       | 1/4 [00:38<01:54, 38.27s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","\t - RQ Train loss: 0.6133\n","\t - RQ Validation Accuracy: 0.7919\n","\t - RQ Validation Precision: 0.8457\n","\t - RQ Validation Recall: 0.7101\n","\t - RQ Validation Specificity: 0.8511\n","\n"]},{"name":"stderr","output_type":"stream","text":["\rEpoch:  50%|     | 2/4 [01:16<01:16, 38.03s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","\t - RQ Train loss: 0.4503\n","\t - RQ Validation Accuracy: 0.7732\n","\t - RQ Validation Precision: 0.7156\n","\t - RQ Validation Recall: 0.8673\n","\t - RQ Validation Specificity: 0.6597\n","\n"]},{"name":"stderr","output_type":"stream","text":["\rEpoch:  75%|  | 3/4 [01:54<00:38, 38.06s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","\t - RQ Train loss: 0.3473\n","\t - RQ Validation Accuracy: 0.7909\n","\t - RQ Validation Precision: 0.7871\n","\t - RQ Validation Recall: 0.7899\n","\t - RQ Validation Specificity: 0.7687\n","\n"]},{"name":"stderr","output_type":"stream","text":["Epoch: 100%|| 4/4 [02:32<00:00, 38.06s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","\t - RQ Train loss: 0.2462\n","\t - RQ Validation Accuracy: 0.7687\n","\t - RQ Validation Precision: 0.7189\n","\t - RQ Validation Recall: 0.8284\n","\t - RQ Validation Specificity: 0.6685\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["model3.cuda()\n","\n","\n","print(\"[INFO:] Training classifier on RQ data...\")\n","\n","for _ in trange(epochs, desc = 'Epoch'):\n","    \n","    # ========== Training ==========\n","    \n","    # Set model to training mode\n","    model3.train()\n","    \n","    # Tracking variables\n","    tr_loss = 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","\n","    for step, batch in enumerate(train_dataloaderR):\n","        batch = tuple(t.to(device) for t in batch)\n","        d_input_ids, d_input_mask, d_labels = batch\n","        optimizer3.zero_grad()\n","        # Forward pass\n","        train_output_RQ = model3(d_input_ids, \n","                             token_type_ids = None, \n","                             attention_mask = d_input_mask, \n","                             labels = d_labels)\n","        # Backward pass\n","        train_output_RQ.loss.backward()\n","        optimizer3.step()\n","        # Update tracking variables\n","        tr_loss += train_output_RQ.loss.item()\n","        nb_tr_examples += d_input_ids.size(0)\n","        nb_tr_steps += 1\n","\n","# ========== Validation ==========\n","\n","    # Set model to evaluation mode\n","    model3.eval()\n","\n","    # Tracking variables \n","    val_accuracy = []\n","    val_precision = []\n","    val_recall = []\n","    val_specificity = []\n","\n","    for batch in validation_dataloaderR:\n","        batch = tuple(t.to(device) for t in batch)\n","        d_input_ids, d_input_mask, d_labels = batch\n","        with torch.no_grad():\n","          # Forward pass\n","          eval_output_RQ = model3(d_input_ids, \n","                              token_type_ids = None, \n","                              attention_mask = d_input_mask)\n","        logits_RQ = eval_output_RQ.logits.detach().cpu().numpy()\n","        label_ids = d_labels.to('cpu').numpy()\n","        # Calculate validation metrics\n","        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits_RQ, label_ids)\n","        val_accuracy.append(b_accuracy)\n","        # Update precision only when (tp + fp) !=0; ignore nan\n","        if b_precision != 'nan': val_precision.append(b_precision)\n","        # Update recall only when (tp + fn) !=0; ignore nan\n","        if b_recall != 'nan': val_recall.append(b_recall)\n","        # Update specificity only when (tn + fp) !=0; ignore nan\n","        if b_specificity != 'nan': val_specificity.append(b_specificity)\n","\n","    print('\\n\\t - RQ Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n","    print('\\t - RQ Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n","    print('\\t - RQ Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n","    print('\\t - RQ Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n","    print('\\t - RQ Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')\n"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":4693,"status":"ok","timestamp":1671205330484,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"tJsps8zk-p0I"},"outputs":[],"source":["# Save the entire model to a file (model1)\n","torch.save({'model_state_dict': model3.state_dict(),\n","            'optimizer_state_dict': optimizer3.state_dict()}, 'model3.pth')"]},{"cell_type":"markdown","metadata":{"id":"OKAXqI5OWlGe"},"source":["Train model on ALL DATA"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":152437,"status":"ok","timestamp":1671205482911,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"Wt-m_LFdWnoa","outputId":"c2f80217-2cc9-4da0-cf9c-36461a0c8330"},"outputs":[{"name":"stdout","output_type":"stream","text":["[INFO:] Training classifier on ALL data...\n"]},{"name":"stderr","output_type":"stream","text":["Epoch:  25%|       | 1/4 [00:38<01:54, 38.18s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","\t - Train loss: 0.6067\n","\t - Validation Accuracy: 0.7707\n","\t - Validation Precision: 0.8012\n","\t - Validation Recall: 0.7361\n","\t - Validation Specificity: 0.7942\n","\n"]},{"name":"stderr","output_type":"stream","text":["\rEpoch:  50%|     | 2/4 [01:16<01:16, 38.00s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","\t - Train loss: 0.4659\n","\t - Validation Accuracy: 0.7641\n","\t - Validation Precision: 0.8306\n","\t - Validation Recall: 0.6740\n","\t - Validation Specificity: 0.8236\n","\n"]},{"name":"stderr","output_type":"stream","text":["\rEpoch:  75%|  | 3/4 [01:54<00:38, 38.00s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","\t - Train loss: 0.3704\n","\t - Validation Accuracy: 0.7854\n","\t - Validation Precision: 0.8015\n","\t - Validation Recall: 0.7553\n","\t - Validation Specificity: 0.7849\n","\n"]},{"name":"stderr","output_type":"stream","text":["Epoch: 100%|| 4/4 [02:32<00:00, 38.04s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","\t - Train loss: 0.2650\n","\t - Validation Accuracy: 0.7631\n","\t - Validation Precision: 0.7164\n","\t - Validation Recall: 0.8610\n","\t - Validation Specificity: 0.6344\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["model4.cuda()\n","\n","print(\"[INFO:] Training classifier on ALL data...\")\n","\n","for _ in trange(epochs, desc = 'Epoch'):\n","    \n","    # ========== Training ==========\n","    \n","    # Set model to training mode\n","    model4.train()\n","    \n","    # Tracking variables\n","    tr_loss = 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","\n","    for step, batch in enumerate(train_dataloaderR):\n","        batch = tuple(t.to(device) for t in batch)\n","        e_input_ids, e_input_mask, e_labels = batch\n","        optimizer4.zero_grad()\n","        # Forward pass\n","        train_output_ALL = model4(e_input_ids, \n","                             token_type_ids = None, \n","                             attention_mask = e_input_mask, \n","                             labels = e_labels)\n","        # Backward pass\n","        train_output_ALL.loss.backward()\n","        optimizer4.step()\n","        # Update tracking variables\n","        tr_loss += train_output_ALL.loss.item()\n","        nb_tr_examples += e_input_ids.size(0)\n","        nb_tr_steps += 1\n","\n","# ========== Validation ==========\n","\n","    # Set model to evaluation mode\n","    model4.eval()\n","\n","    # Tracking variables \n","    val_accuracy = []\n","    val_precision = []\n","    val_recall = []\n","    val_specificity = []\n","\n","    for batch in validation_dataloaderR:\n","        batch = tuple(t.to(device) for t in batch)\n","        e_input_ids, e_input_mask, e_labels = batch\n","        with torch.no_grad():\n","          # Forward pass\n","          eval_output_ALL = model4(e_input_ids, \n","                              token_type_ids = None, \n","                              attention_mask = e_input_mask)\n","        logits_ALL = eval_output_ALL.logits.detach().cpu().numpy()\n","        label_ids = e_labels.to('cpu').numpy()\n","        # Calculate validation metrics\n","        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits_ALL, label_ids)\n","        val_accuracy.append(b_accuracy)\n","        # Update precision only when (tp + fp) !=0; ignore nan\n","        if b_precision != 'nan': val_precision.append(b_precision)\n","        # Update recall only when (tp + fn) !=0; ignore nan\n","        if b_recall != 'nan': val_recall.append(b_recall)\n","        # Update specificity only when (tn + fp) !=0; ignore nan\n","        if b_specificity != 'nan': val_specificity.append(b_specificity)\n","\n","    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n","    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n","    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n","    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n","    print('\\t - Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')\n"]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":5086,"status":"ok","timestamp":1671205487985,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"rTuuZv7l-046"},"outputs":[],"source":["# Save the entire model to a file (model1)\n","torch.save({'model_state_dict': model4.state_dict(),\n","            'optimizer_state_dict': optimizer4.state_dict()}, 'model4.pth')"]},{"cell_type":"markdown","metadata":{"id":"b5AvehZmOnPZ"},"source":["Create plot of loss history if this is needed\n"]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1671205487988,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"SrHe8p8POnPa"},"outputs":[],"source":["# def create_plot(tr_loss):\n","#     fig, ax = plt.subplots()\n","#     ax.plot(loss_history, label = \"training loss\")\n","#     plt.legend()\n","#     plt.show()\n","#     #plot_outpath = os.path.join(\"loss.png\")\n","#     #fig.savefig(plot_outpath) \n","#     return \n","\n","# create_plot(tr_loss)\n","\n","#epoch1 = (0.5135, 0.8740, 0.8638, 0.8912, 0.8516)\n","#epoch2 = "]},{"cell_type":"markdown","metadata":{"id":"iNdR4KRZnsr7"},"source":["Now I test the model ?"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1671205487989,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"foEHLuaKm0YB"},"outputs":[],"source":["# Test set\n","test_setG = TensorDataset(test_id_G, \n","                        test_masks_G, \n","                        labelsGEN_test)\n","\n","# Prepare dataloader\n","\n","test_dataloaderG = DataLoader(\n","            test_setG,\n","            sampler = SequentialSampler(test_setG),\n","            batch_size = batch_size\n","        )\n","\n"]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1671205487990,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"f-b4Scz50QQ8"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":415},"executionInfo":{"elapsed":11,"status":"error","timestamp":1671205487990,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"iFwS05fUAolR","outputId":"404875ef-22fd-4773-c7d3-bcccbf237240"},"outputs":[{"ename":"RuntimeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-43-84250d68fc40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m  \u001b[0;31m# Use the model to make a prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_id_G\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_masks_G\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1567\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m   1015\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2208\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)"]}],"source":["# # Set the model to evaluation mode\n","# model1.eval()\n","\n","# # Initialize the list to store the predictions\n","# predictionsGEN = []\n","\n","#  # Use the model to make a prediction\n","# with torch.no_grad():\n","#         output = model1(test_id_G, test_masks_G)\n","#         _, prediction = torch.max(output, dim=1)\n","\n","#     # Add the prediction to the list\n","# predictionsGEN.append(prediction.item())\n","\n","# # Calculate the accuracy of the predictions\n","# accuracy = sum(predictionsGEN == labelsGEN_test) / len(labelsGEN_test)\n","# print(f'Accuracy: {accuracy:.2f}')"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":270},"executionInfo":{"elapsed":1722,"status":"error","timestamp":1671207554644,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"rLx1AesjD76U","outputId":"ceeefaff-1ee2-43fc-b596-8d624730c992"},"outputs":[{"ename":"AttributeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-44-cd5e12424c16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_id_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_masks_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Sarcastic'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'Not sarcastic'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'logits'"]}],"source":["# predictions = []\n","\n","# with torch.no_grad():\n","#   output = model1(test_id_G.to(device), token_type_ids = None, attention_mask = test_masks_G.to(device)),\n","#   prediction = 'Sarcastic' if np.argmax(output.logits.cpu().numpy()).flatten().item() == 1 else 'Not sarcastic'\n","#   predictions.append(prediction.item())\n","\n","# print('Predicted Class: ', prediction) \n","\n","# accuracy = sum(predictions == labelsGEN_test) / len(labelsGEN_test)\n","# print(f'Accuracy: {accuracy:.2f}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"aborted","timestamp":1671205487992,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"9V66qBr2lkEv"},"outputs":[],"source":["# # Plot\n","# predicted = model1(X_vect).detach().numpy()\n","# print(classification_report(y, \n","#                             np.where(predicted > 0.5, 1, 0),\n","#                             target_names = [\"Negative\", \"Positive\"]))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"aborted","timestamp":1671205487992,"user":{"displayName":"Gacilda Anne","userId":"15459045031671243554"},"user_tz":-60},"id":"QaHsFmcnkren"},"outputs":[],"source":["# pred = model1.predict(test_id_G)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.4 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.4"},"vscode":{"interpreter":{"hash":"33c2a87c6748a455dcd2d018de6a7096e8fcfc12759846481f65033bc9113f88"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"0728c01761374afe963e86eba8fab3ae":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09e6b8cd29b34897a898973508febef1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_11623e2c1c8d4efd8534f1c224c75a30","placeholder":"","style":"IPY_MODEL_ee053d61bdda439f84f37333fd3a2306","value":" 28.0/28.0 [00:00&lt;00:00, 1.37kB/s]"}},"11623e2c1c8d4efd8534f1c224c75a30":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"181160ca9c7b4e27ab8539fcec68d7fb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21274bd2d6684e5aafd785871599f171":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2a58fac03b184dc3ae88c4e22ecc50f8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a5d6ae85cb845b3b9933c35b1a5501a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ac7c3514813f404a9f1fc87a04e88c57","IPY_MODEL_78dd9cff75b54f2a8967a61fd657e840","IPY_MODEL_91cd6260e1c146eaab28a1fe90a0250b"],"layout":"IPY_MODEL_e98c9f6fb08b43c3b75bb28b7ed89683"}},"3c9f4a7bae40438e8a08b3df301baf06":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3fa92f9f57d741f08fc6054aba22b7f9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"562f8eca86da44e282a55d8442afac92":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a58fac03b184dc3ae88c4e22ecc50f8","placeholder":"","style":"IPY_MODEL_973776c0b5ec45d0a2ae180534611df1","value":"Downloading: 100%"}},"56d5c9265bbf487088e7e9f15396b05d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"78dd9cff75b54f2a8967a61fd657e840":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8f0702a5fc143959f2fdfb4ce0a4130","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3c9f4a7bae40438e8a08b3df301baf06","value":231508}},"83bf836aa925473d831aee36f5031ef2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_562f8eca86da44e282a55d8442afac92","IPY_MODEL_f19724243b7b46f6bf07aac17e57f5a4","IPY_MODEL_09e6b8cd29b34897a898973508febef1"],"layout":"IPY_MODEL_8b4fcd200a7b4e3bb9e11938c3b0cdcb"}},"8a5bb0a0ea3e40f196f10e6f72136cf5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b4fcd200a7b4e3bb9e11938c3b0cdcb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91cd6260e1c146eaab28a1fe90a0250b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0728c01761374afe963e86eba8fab3ae","placeholder":"","style":"IPY_MODEL_9547a27d702148ddbde6ec4ed8dbcfea","value":" 232k/232k [00:00&lt;00:00, 908kB/s]"}},"9547a27d702148ddbde6ec4ed8dbcfea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"973776c0b5ec45d0a2ae180534611df1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"99a0d8bade97483b8df7481da1604ef4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3fa92f9f57d741f08fc6054aba22b7f9","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_21274bd2d6684e5aafd785871599f171","value":570}},"9f54c3c2bea441f289ea1c5f5eb3ab2a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cad6ef30082040669140f737018db588","placeholder":"","style":"IPY_MODEL_a7721104836a436da78c222207d54b06","value":"Downloading: 100%"}},"a52d83281e604b748953668b4b66a9f9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7721104836a436da78c222207d54b06":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a7924494dd4a469096b1e6316746843d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a7c966949a084c75a02b3113671949e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9f54c3c2bea441f289ea1c5f5eb3ab2a","IPY_MODEL_99a0d8bade97483b8df7481da1604ef4","IPY_MODEL_c1f8c82574e54d1cba1de934843d9a48"],"layout":"IPY_MODEL_181160ca9c7b4e27ab8539fcec68d7fb"}},"a8f0702a5fc143959f2fdfb4ce0a4130":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac7c3514813f404a9f1fc87a04e88c57":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a52d83281e604b748953668b4b66a9f9","placeholder":"","style":"IPY_MODEL_56d5c9265bbf487088e7e9f15396b05d","value":"Downloading: 100%"}},"c1f8c82574e54d1cba1de934843d9a48":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2347bab25ea4af8ac364ca596d893b9","placeholder":"","style":"IPY_MODEL_a7924494dd4a469096b1e6316746843d","value":" 570/570 [00:00&lt;00:00, 16.4kB/s]"}},"cad6ef30082040669140f737018db588":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e98c9f6fb08b43c3b75bb28b7ed89683":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee053d61bdda439f84f37333fd3a2306":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f19724243b7b46f6bf07aac17e57f5a4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a5bb0a0ea3e40f196f10e6f72136cf5","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fdf41ea32c50487a90baacf28cd75ca6","value":28}},"f2347bab25ea4af8ac364ca596d893b9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdf41ea32c50487a90baacf28cd75ca6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
